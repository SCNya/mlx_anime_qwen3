{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26fa358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vsevolod/mlx_anime_qwen3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 67963.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>\n",
      "Okay, so the user is asking how many members the anime \"Peach Girl\" has. Let me start by recalling what I know about this series. I remember that \"Peach Girl\" is a popular Japanese anime, and it's known for its unique characters and story. The main characters are the protagonist, a girl named Peach, and her friends. But wait, I think there are more characters involved. Let me think... Oh right, there's also the character of the girl who is the main character of the story, and then there's the other main character, maybe a friend or a sibling. Wait, I'm getting confused. Let me check my memory again.\n",
      "\n",
      "I think the main characters are Peach, her friend, and another girl. But I'm not entirely sure. Maybe there are more. Oh, right! The anime has a main protagonist, a main character, and a few other characters. Let me confirm. The original series has the main characters: Peach, her friend, and another girl. So, that's three main characters. But I'm not 100% sure. Maybe there are more. Alternatively, perhaps the answer is that it has three main characters. But I need to be careful here. Let me think again. The original series is called \"Peach Girl,\" and it's known for its three main characters: Peach, her friend, and another girl. So, the answer would be three. But I should make sure. Alternatively, maybe there are more. For example, the anime has a main protagonist, a main character, and a few others. Let me check my notes. Yes, I think the answer is three. So, the response should state that \"Peach Girl\" has three main members.\n",
      "</think>\n",
      "\n",
      "\"Peach Girl\" has three main members: the protagonist Peach, her friend, and another character who plays a central role in the story.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Загрузка модели и токенизатора с MLX\n",
    "model, tokenizer = load(\"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "# Шаблон для обучающих данных\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are an expert in Japanese animation with advanced knowledge in anime, manga, Japanese cartoons and animated films.\n",
    "Please answer the following anime fan question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\"\n",
    "\n",
    "# Функция для форматирования примеров\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    complex_cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for question, cot, response in zip(inputs, complex_cots, outputs):\n",
    "        # Добавляем EOS-токен к ответу, если его нет\n",
    "        if not response.endswith(tokenizer.eos_token):\n",
    "            response += tokenizer.eos_token\n",
    "        text = train_prompt_style.format(question, cot, response)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Загрузка и обработка датасета\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"datasets-anime-sharegpt-2025-05-24.json\",\n",
    "    split=\"train\"\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Шаблон для инференса\n",
    "inference_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are an expert in Japanese animation with advanced knowledge in anime, manga, Japanese cartoons and animated films.\n",
    "Please answer the following anime fan question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "\"\"\"\n",
    "\n",
    "# Выполнение инференса\n",
    "question = dataset[10]['Question']\n",
    "prompt = inference_prompt_style.format(question)\n",
    "\n",
    "outputs = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_tokens=1200\n",
    ")\n",
    "\n",
    "outputs = generate(model, tokenizer, prompt, max_tokens=1200)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042dd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c35f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe7263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2621440\n",
      "Starting training..., iters: 2930\n",
      "Iter 1: Val loss 2.430, Val took 4.963s\n",
      "Iter 10: Train loss 1.692, Learning Rate 1.000e-05, It/sec 2.521, Tokens/sec 1190.982, Trained Tokens 4725, Peak mem 2.699 GB\n",
      "Iter 20: Train loss 1.527, Learning Rate 1.000e-05, It/sec 2.108, Tokens/sec 1221.345, Trained Tokens 10518, Peak mem 4.480 GB\n",
      "Iter 30: Train loss 1.347, Learning Rate 1.000e-05, It/sec 2.442, Tokens/sec 1258.191, Trained Tokens 15671, Peak mem 4.480 GB\n",
      "Iter 40: Train loss 1.090, Learning Rate 1.000e-05, It/sec 3.090, Tokens/sec 1254.796, Trained Tokens 19732, Peak mem 4.480 GB\n",
      "Iter 50: Val loss 1.053, Val took 4.948s\n",
      "Iter 50: Train loss 1.168, Learning Rate 1.000e-05, It/sec 2.458, Tokens/sec 1260.532, Trained Tokens 24861, Peak mem 4.480 GB\n",
      "Iter 60: Train loss 1.144, Learning Rate 1.000e-05, It/sec 2.988, Tokens/sec 1264.103, Trained Tokens 29091, Peak mem 4.480 GB\n",
      "Iter 70: Train loss 1.106, Learning Rate 1.000e-05, It/sec 2.567, Tokens/sec 1234.985, Trained Tokens 33902, Peak mem 4.480 GB\n",
      "Iter 80: Train loss 1.118, Learning Rate 1.000e-05, It/sec 2.745, Tokens/sec 1273.748, Trained Tokens 38543, Peak mem 4.480 GB\n",
      "Iter 90: Train loss 1.014, Learning Rate 1.000e-05, It/sec 2.736, Tokens/sec 1250.214, Trained Tokens 43112, Peak mem 4.480 GB\n",
      "Iter 100: Val loss 1.068, Val took 5.344s\n",
      "Iter 100: Train loss 0.974, Learning Rate 1.000e-05, It/sec 2.825, Tokens/sec 1270.630, Trained Tokens 47610, Peak mem 4.480 GB\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 1.038, Learning Rate 1.000e-05, It/sec 2.644, Tokens/sec 1237.515, Trained Tokens 52291, Peak mem 4.480 GB\n",
      "Iter 120: Train loss 1.032, Learning Rate 1.000e-05, It/sec 2.519, Tokens/sec 1253.243, Trained Tokens 57266, Peak mem 4.480 GB\n",
      "Iter 130: Train loss 0.989, Learning Rate 1.000e-05, It/sec 2.702, Tokens/sec 1246.886, Trained Tokens 61880, Peak mem 4.480 GB\n",
      "Iter 140: Train loss 1.006, Learning Rate 1.000e-05, It/sec 2.669, Tokens/sec 1260.368, Trained Tokens 66602, Peak mem 4.480 GB\n",
      "Iter 150: Val loss 0.997, Val took 5.230s\n",
      "Iter 150: Train loss 1.190, Learning Rate 1.000e-05, It/sec 2.573, Tokens/sec 1264.046, Trained Tokens 71515, Peak mem 4.480 GB\n",
      "Iter 160: Train loss 1.093, Learning Rate 1.000e-05, It/sec 2.369, Tokens/sec 1262.517, Trained Tokens 76845, Peak mem 4.480 GB\n",
      "Iter 170: Train loss 1.077, Learning Rate 1.000e-05, It/sec 2.252, Tokens/sec 1243.726, Trained Tokens 82368, Peak mem 4.480 GB\n",
      "Iter 180: Train loss 1.021, Learning Rate 1.000e-05, It/sec 2.894, Tokens/sec 1290.304, Trained Tokens 86826, Peak mem 4.480 GB\n",
      "Iter 190: Train loss 1.006, Learning Rate 1.000e-05, It/sec 2.569, Tokens/sec 1265.992, Trained Tokens 91753, Peak mem 4.480 GB\n",
      "Iter 200: Val loss 0.858, Val took 5.091s\n",
      "Iter 200: Train loss 1.072, Learning Rate 1.000e-05, It/sec 2.411, Tokens/sec 1256.757, Trained Tokens 96966, Peak mem 4.480 GB\n",
      "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.877, Learning Rate 1.000e-05, It/sec 2.883, Tokens/sec 1231.498, Trained Tokens 101237, Peak mem 4.480 GB\n",
      "Iter 220: Train loss 1.011, Learning Rate 1.000e-05, It/sec 2.702, Tokens/sec 1284.948, Trained Tokens 105992, Peak mem 4.480 GB\n",
      "Iter 230: Train loss 0.916, Learning Rate 1.000e-05, It/sec 2.486, Tokens/sec 1246.230, Trained Tokens 111005, Peak mem 4.480 GB\n",
      "Iter 240: Train loss 1.082, Learning Rate 1.000e-05, It/sec 2.356, Tokens/sec 1254.028, Trained Tokens 116327, Peak mem 4.480 GB\n",
      "Iter 250: Val loss 0.922, Val took 4.939s\n",
      "Iter 250: Train loss 1.008, Learning Rate 1.000e-05, It/sec 2.659, Tokens/sec 1244.866, Trained Tokens 121009, Peak mem 4.480 GB\n",
      "Iter 260: Train loss 0.999, Learning Rate 1.000e-05, It/sec 2.741, Tokens/sec 1283.713, Trained Tokens 125692, Peak mem 4.480 GB\n",
      "Iter 270: Train loss 1.069, Learning Rate 1.000e-05, It/sec 2.267, Tokens/sec 1252.759, Trained Tokens 131218, Peak mem 4.480 GB\n",
      "Iter 280: Train loss 1.028, Learning Rate 1.000e-05, It/sec 2.774, Tokens/sec 1254.114, Trained Tokens 135739, Peak mem 4.480 GB\n",
      "Iter 290: Train loss 0.973, Learning Rate 1.000e-05, It/sec 2.858, Tokens/sec 1291.690, Trained Tokens 140258, Peak mem 4.480 GB\n",
      "Iter 300: Val loss 0.791, Val took 5.386s\n",
      "Iter 300: Train loss 0.900, Learning Rate 1.000e-05, It/sec 2.604, Tokens/sec 1276.423, Trained Tokens 145160, Peak mem 4.480 GB\n",
      "Iter 300: Saved adapter weights to adapters/adapters.safetensors and adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.761, Learning Rate 1.000e-05, It/sec 2.611, Tokens/sec 1232.707, Trained Tokens 149882, Peak mem 4.480 GB\n",
      "Iter 320: Train loss 0.778, Learning Rate 1.000e-05, It/sec 2.721, Tokens/sec 1249.060, Trained Tokens 154472, Peak mem 4.480 GB\n",
      "Iter 330: Train loss 0.720, Learning Rate 1.000e-05, It/sec 2.586, Tokens/sec 1252.864, Trained Tokens 159316, Peak mem 4.480 GB\n",
      "Iter 340: Train loss 0.795, Learning Rate 1.000e-05, It/sec 2.389, Tokens/sec 1252.399, Trained Tokens 164559, Peak mem 4.480 GB\n",
      "Iter 350: Val loss 0.829, Val took 4.925s\n",
      "Iter 350: Train loss 0.751, Learning Rate 1.000e-05, It/sec 2.577, Tokens/sec 1240.415, Trained Tokens 169373, Peak mem 4.480 GB\n",
      "Iter 360: Train loss 0.752, Learning Rate 1.000e-05, It/sec 2.782, Tokens/sec 1283.960, Trained Tokens 173989, Peak mem 4.480 GB\n",
      "Iter 370: Train loss 0.735, Learning Rate 1.000e-05, It/sec 2.823, Tokens/sec 1276.492, Trained Tokens 178510, Peak mem 4.480 GB\n",
      "Iter 380: Train loss 0.851, Learning Rate 1.000e-05, It/sec 2.435, Tokens/sec 1235.623, Trained Tokens 183584, Peak mem 4.480 GB\n",
      "Iter 390: Train loss 0.832, Learning Rate 1.000e-05, It/sec 2.406, Tokens/sec 1224.571, Trained Tokens 188674, Peak mem 4.490 GB\n",
      "Iter 400: Val loss 0.814, Val took 5.289s\n",
      "Iter 400: Train loss 0.815, Learning Rate 1.000e-05, It/sec 2.482, Tokens/sec 1261.849, Trained Tokens 193758, Peak mem 4.490 GB\n",
      "Iter 400: Saved adapter weights to adapters/adapters.safetensors and adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.873, Learning Rate 1.000e-05, It/sec 2.398, Tokens/sec 1249.963, Trained Tokens 198971, Peak mem 4.490 GB\n",
      "Iter 420: Train loss 0.718, Learning Rate 1.000e-05, It/sec 2.667, Tokens/sec 1263.715, Trained Tokens 203710, Peak mem 4.490 GB\n",
      "Iter 430: Train loss 0.757, Learning Rate 1.000e-05, It/sec 2.692, Tokens/sec 1246.132, Trained Tokens 208339, Peak mem 4.490 GB\n",
      "Iter 440: Train loss 0.691, Learning Rate 1.000e-05, It/sec 2.795, Tokens/sec 1250.855, Trained Tokens 212814, Peak mem 4.490 GB\n",
      "Iter 450: Val loss 0.691, Val took 4.792s\n",
      "Iter 450: Train loss 0.816, Learning Rate 1.000e-05, It/sec 2.785, Tokens/sec 1262.115, Trained Tokens 217346, Peak mem 4.490 GB\n",
      "Iter 460: Train loss 0.853, Learning Rate 1.000e-05, It/sec 2.611, Tokens/sec 1290.025, Trained Tokens 222286, Peak mem 4.490 GB\n",
      "Iter 470: Train loss 0.865, Learning Rate 1.000e-05, It/sec 2.528, Tokens/sec 1274.019, Trained Tokens 227326, Peak mem 4.490 GB\n",
      "Iter 480: Train loss 0.781, Learning Rate 1.000e-05, It/sec 2.578, Tokens/sec 1229.886, Trained Tokens 232097, Peak mem 4.490 GB\n",
      "Iter 490: Train loss 0.876, Learning Rate 1.000e-05, It/sec 2.962, Tokens/sec 1289.258, Trained Tokens 236449, Peak mem 4.490 GB\n",
      "Iter 500: Val loss 0.653, Val took 4.652s\n",
      "Iter 500: Train loss 0.780, Learning Rate 1.000e-05, It/sec 2.727, Tokens/sec 1276.678, Trained Tokens 241130, Peak mem 4.490 GB\n",
      "Iter 500: Saved adapter weights to adapters/adapters.safetensors and adapters/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 0.820, Learning Rate 1.000e-05, It/sec 2.686, Tokens/sec 1244.722, Trained Tokens 245764, Peak mem 4.490 GB\n",
      "Iter 520: Train loss 0.830, Learning Rate 1.000e-05, It/sec 2.426, Tokens/sec 1238.664, Trained Tokens 250870, Peak mem 4.490 GB\n",
      "Iter 530: Train loss 0.799, Learning Rate 1.000e-05, It/sec 2.400, Tokens/sec 1264.664, Trained Tokens 256140, Peak mem 4.490 GB\n",
      "Iter 540: Train loss 0.770, Learning Rate 1.000e-05, It/sec 2.700, Tokens/sec 1226.365, Trained Tokens 260682, Peak mem 4.490 GB\n",
      "Iter 550: Val loss 0.682, Val took 4.965s\n",
      "Iter 550: Train loss 0.685, Learning Rate 1.000e-05, It/sec 2.901, Tokens/sec 1232.670, Trained Tokens 264931, Peak mem 4.490 GB\n",
      "Iter 560: Train loss 0.802, Learning Rate 1.000e-05, It/sec 2.558, Tokens/sec 1251.173, Trained Tokens 269822, Peak mem 4.490 GB\n",
      "Iter 570: Train loss 0.866, Learning Rate 1.000e-05, It/sec 2.588, Tokens/sec 1259.816, Trained Tokens 274690, Peak mem 4.490 GB\n",
      "Iter 580: Train loss 0.760, Learning Rate 1.000e-05, It/sec 2.628, Tokens/sec 1251.394, Trained Tokens 279452, Peak mem 4.490 GB\n",
      "Iter 590: Train loss 0.852, Learning Rate 1.000e-05, It/sec 2.129, Tokens/sec 1246.551, Trained Tokens 285308, Peak mem 4.490 GB\n",
      "Iter 600: Val loss 0.650, Val took 5.025s\n",
      "Iter 600: Train loss 0.639, Learning Rate 1.000e-05, It/sec 2.533, Tokens/sec 1261.630, Trained Tokens 290289, Peak mem 4.490 GB\n",
      "Iter 600: Saved adapter weights to adapters/adapters.safetensors and adapters/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 0.637, Learning Rate 1.000e-05, It/sec 2.842, Tokens/sec 1261.374, Trained Tokens 294728, Peak mem 4.490 GB\n",
      "Iter 620: Train loss 0.616, Learning Rate 1.000e-05, It/sec 2.736, Tokens/sec 1280.959, Trained Tokens 299410, Peak mem 4.490 GB\n",
      "Iter 630: Train loss 0.694, Learning Rate 1.000e-05, It/sec 2.492, Tokens/sec 1285.317, Trained Tokens 304568, Peak mem 4.490 GB\n",
      "Iter 640: Train loss 0.484, Learning Rate 1.000e-05, It/sec 3.138, Tokens/sec 1253.570, Trained Tokens 308563, Peak mem 4.490 GB\n",
      "Iter 650: Val loss 0.668, Val took 5.301s\n",
      "Iter 650: Train loss 0.614, Learning Rate 1.000e-05, It/sec 2.644, Tokens/sec 1216.229, Trained Tokens 313163, Peak mem 4.490 GB\n",
      "Iter 660: Train loss 0.563, Learning Rate 1.000e-05, It/sec 2.676, Tokens/sec 1234.145, Trained Tokens 317775, Peak mem 4.490 GB\n",
      "Iter 670: Train loss 0.723, Learning Rate 1.000e-05, It/sec 2.269, Tokens/sec 1252.439, Trained Tokens 323296, Peak mem 4.490 GB\n",
      "Iter 680: Train loss 0.587, Learning Rate 1.000e-05, It/sec 3.045, Tokens/sec 1277.870, Trained Tokens 327492, Peak mem 4.490 GB\n",
      "Iter 690: Train loss 0.563, Learning Rate 1.000e-05, It/sec 2.925, Tokens/sec 1266.612, Trained Tokens 331823, Peak mem 4.490 GB\n",
      "Iter 700: Val loss 0.561, Val took 5.233s\n",
      "Iter 700: Train loss 0.596, Learning Rate 1.000e-05, It/sec 2.714, Tokens/sec 1233.730, Trained Tokens 336368, Peak mem 4.490 GB\n",
      "Iter 700: Saved adapter weights to adapters/adapters.safetensors and adapters/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 0.711, Learning Rate 1.000e-05, It/sec 2.237, Tokens/sec 1231.676, Trained Tokens 341873, Peak mem 4.490 GB\n",
      "Iter 720: Train loss 0.716, Learning Rate 1.000e-05, It/sec 2.313, Tokens/sec 1240.806, Trained Tokens 347237, Peak mem 4.490 GB\n",
      "Iter 730: Train loss 0.698, Learning Rate 1.000e-05, It/sec 2.496, Tokens/sec 1232.962, Trained Tokens 352176, Peak mem 4.490 GB\n",
      "Iter 740: Train loss 0.627, Learning Rate 1.000e-05, It/sec 2.540, Tokens/sec 1253.815, Trained Tokens 357113, Peak mem 4.490 GB\n",
      "Iter 750: Val loss 0.591, Val took 5.107s\n",
      "Iter 750: Train loss 0.705, Learning Rate 1.000e-05, It/sec 2.265, Tokens/sec 1245.605, Trained Tokens 362612, Peak mem 4.490 GB\n",
      "Iter 760: Train loss 0.617, Learning Rate 1.000e-05, It/sec 2.731, Tokens/sec 1283.768, Trained Tokens 367312, Peak mem 4.490 GB\n",
      "Iter 770: Train loss 0.642, Learning Rate 1.000e-05, It/sec 2.874, Tokens/sec 1257.756, Trained Tokens 371689, Peak mem 4.490 GB\n",
      "Iter 780: Train loss 0.595, Learning Rate 1.000e-05, It/sec 2.873, Tokens/sec 1263.063, Trained Tokens 376085, Peak mem 4.490 GB\n",
      "Iter 790: Train loss 0.720, Learning Rate 1.000e-05, It/sec 2.485, Tokens/sec 1252.209, Trained Tokens 381124, Peak mem 4.490 GB\n",
      "Iter 800: Val loss 0.547, Val took 4.689s\n",
      "Iter 800: Train loss 0.539, Learning Rate 1.000e-05, It/sec 2.881, Tokens/sec 1275.098, Trained Tokens 385550, Peak mem 4.490 GB\n",
      "Iter 800: Saved adapter weights to adapters/adapters.safetensors and adapters/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 0.673, Learning Rate 1.000e-05, It/sec 2.581, Tokens/sec 1263.424, Trained Tokens 390446, Peak mem 4.490 GB\n",
      "Iter 820: Train loss 0.698, Learning Rate 1.000e-05, It/sec 2.317, Tokens/sec 1251.961, Trained Tokens 395849, Peak mem 4.490 GB\n",
      "Iter 830: Train loss 0.719, Learning Rate 1.000e-05, It/sec 2.415, Tokens/sec 1278.682, Trained Tokens 401144, Peak mem 4.490 GB\n",
      "Iter 840: Train loss 0.567, Learning Rate 1.000e-05, It/sec 2.602, Tokens/sec 1231.556, Trained Tokens 405878, Peak mem 4.490 GB\n",
      "Iter 850: Val loss 0.522, Val took 4.990s\n",
      "Iter 850: Train loss 0.696, Learning Rate 1.000e-05, It/sec 2.428, Tokens/sec 1250.191, Trained Tokens 411028, Peak mem 4.490 GB\n",
      "Iter 860: Train loss 0.645, Learning Rate 1.000e-05, It/sec 2.793, Tokens/sec 1271.594, Trained Tokens 415580, Peak mem 4.490 GB\n",
      "Iter 870: Train loss 0.578, Learning Rate 1.000e-05, It/sec 2.947, Tokens/sec 1259.406, Trained Tokens 419853, Peak mem 4.490 GB\n",
      "Iter 880: Train loss 0.614, Learning Rate 1.000e-05, It/sec 2.687, Tokens/sec 1261.335, Trained Tokens 424548, Peak mem 4.490 GB\n",
      "Iter 890: Train loss 0.461, Learning Rate 1.000e-05, It/sec 2.870, Tokens/sec 1269.538, Trained Tokens 428971, Peak mem 4.490 GB\n",
      "Iter 900: Val loss 0.513, Val took 5.160s\n",
      "Iter 900: Train loss 0.489, Learning Rate 1.000e-05, It/sec 2.555, Tokens/sec 1250.610, Trained Tokens 433866, Peak mem 4.490 GB\n",
      "Iter 900: Saved adapter weights to adapters/adapters.safetensors and adapters/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 0.466, Learning Rate 1.000e-05, It/sec 2.534, Tokens/sec 1261.091, Trained Tokens 438843, Peak mem 4.490 GB\n",
      "Iter 920: Train loss 0.362, Learning Rate 1.000e-05, It/sec 3.262, Tokens/sec 1258.243, Trained Tokens 442700, Peak mem 4.490 GB\n",
      "Iter 930: Train loss 0.489, Learning Rate 1.000e-05, It/sec 2.492, Tokens/sec 1235.180, Trained Tokens 447656, Peak mem 4.490 GB\n",
      "Iter 940: Train loss 0.483, Learning Rate 1.000e-05, It/sec 2.576, Tokens/sec 1243.690, Trained Tokens 452484, Peak mem 4.490 GB\n",
      "Iter 950: Val loss 0.531, Val took 5.287s\n",
      "Iter 950: Train loss 0.534, Learning Rate 1.000e-05, It/sec 2.605, Tokens/sec 1242.937, Trained Tokens 457255, Peak mem 4.490 GB\n",
      "Iter 960: Train loss 0.484, Learning Rate 1.000e-05, It/sec 2.673, Tokens/sec 1227.127, Trained Tokens 461846, Peak mem 4.490 GB\n",
      "Iter 970: Train loss 0.439, Learning Rate 1.000e-05, It/sec 2.665, Tokens/sec 1243.170, Trained Tokens 466511, Peak mem 4.490 GB\n",
      "Iter 980: Train loss 0.508, Learning Rate 1.000e-05, It/sec 2.805, Tokens/sec 1289.530, Trained Tokens 471109, Peak mem 4.490 GB\n",
      "Iter 990: Train loss 0.403, Learning Rate 1.000e-05, It/sec 2.684, Tokens/sec 1235.287, Trained Tokens 475711, Peak mem 4.490 GB\n",
      "Iter 1000: Val loss 0.401, Val took 4.941s\n",
      "Iter 1000: Train loss 0.526, Learning Rate 1.000e-05, It/sec 2.566, Tokens/sec 1266.858, Trained Tokens 480648, Peak mem 4.490 GB\n",
      "Iter 1000: Saved adapter weights to adapters/adapters.safetensors and adapters/0001000_adapters.safetensors.\n",
      "Iter 1010: Train loss 0.501, Learning Rate 1.000e-05, It/sec 2.696, Tokens/sec 1257.304, Trained Tokens 485311, Peak mem 4.490 GB\n",
      "Iter 1020: Train loss 0.525, Learning Rate 1.000e-05, It/sec 2.688, Tokens/sec 1261.353, Trained Tokens 490004, Peak mem 4.490 GB\n",
      "Iter 1030: Train loss 0.594, Learning Rate 1.000e-05, It/sec 2.318, Tokens/sec 1244.653, Trained Tokens 495374, Peak mem 4.490 GB\n",
      "Iter 1040: Train loss 0.570, Learning Rate 1.000e-05, It/sec 2.244, Tokens/sec 1235.718, Trained Tokens 500881, Peak mem 4.490 GB\n",
      "Iter 1050: Val loss 0.412, Val took 4.912s\n",
      "Iter 1050: Train loss 0.508, Learning Rate 1.000e-05, It/sec 2.518, Tokens/sec 1260.450, Trained Tokens 505887, Peak mem 4.490 GB\n",
      "Iter 1060: Train loss 0.471, Learning Rate 1.000e-05, It/sec 2.608, Tokens/sec 1269.316, Trained Tokens 510754, Peak mem 4.490 GB\n",
      "Iter 1070: Train loss 0.482, Learning Rate 1.000e-05, It/sec 2.491, Tokens/sec 1241.350, Trained Tokens 515738, Peak mem 4.490 GB\n",
      "Iter 1080: Train loss 0.490, Learning Rate 1.000e-05, It/sec 2.752, Tokens/sec 1261.143, Trained Tokens 520321, Peak mem 4.490 GB\n",
      "Iter 1090: Train loss 0.492, Learning Rate 1.000e-05, It/sec 2.729, Tokens/sec 1269.458, Trained Tokens 524972, Peak mem 4.490 GB\n",
      "Iter 1100: Val loss 0.412, Val took 4.960s\n",
      "Iter 1100: Train loss 0.445, Learning Rate 1.000e-05, It/sec 2.916, Tokens/sec 1281.789, Trained Tokens 529367, Peak mem 4.490 GB\n",
      "Iter 1100: Saved adapter weights to adapters/adapters.safetensors and adapters/0001100_adapters.safetensors.\n",
      "Iter 1110: Train loss 0.538, Learning Rate 1.000e-05, It/sec 2.702, Tokens/sec 1256.587, Trained Tokens 534017, Peak mem 4.490 GB\n",
      "Iter 1120: Train loss 0.505, Learning Rate 1.000e-05, It/sec 2.618, Tokens/sec 1245.975, Trained Tokens 538776, Peak mem 4.490 GB\n",
      "Iter 1130: Train loss 0.609, Learning Rate 1.000e-05, It/sec 2.286, Tokens/sec 1264.022, Trained Tokens 544305, Peak mem 4.490 GB\n",
      "Iter 1140: Train loss 0.563, Learning Rate 1.000e-05, It/sec 2.473, Tokens/sec 1234.111, Trained Tokens 549295, Peak mem 4.490 GB\n",
      "Iter 1150: Val loss 0.383, Val took 4.984s\n",
      "Iter 1150: Train loss 0.525, Learning Rate 1.000e-05, It/sec 2.535, Tokens/sec 1280.363, Trained Tokens 554345, Peak mem 4.490 GB\n",
      "Iter 1160: Train loss 0.557, Learning Rate 1.000e-05, It/sec 2.326, Tokens/sec 1225.376, Trained Tokens 559614, Peak mem 4.490 GB\n",
      "Iter 1170: Train loss 0.556, Learning Rate 1.000e-05, It/sec 2.442, Tokens/sec 1247.856, Trained Tokens 564723, Peak mem 4.490 GB\n",
      "Iter 1180: Train loss 0.388, Learning Rate 1.000e-05, It/sec 2.725, Tokens/sec 1244.507, Trained Tokens 569290, Peak mem 4.490 GB\n",
      "Iter 1190: Train loss 0.373, Learning Rate 1.000e-05, It/sec 2.567, Tokens/sec 1241.603, Trained Tokens 574127, Peak mem 4.490 GB\n",
      "Iter 1200: Val loss 0.359, Val took 5.161s\n",
      "Iter 1200: Train loss 0.329, Learning Rate 1.000e-05, It/sec 2.835, Tokens/sec 1275.812, Trained Tokens 578627, Peak mem 4.490 GB\n",
      "Iter 1200: Saved adapter weights to adapters/adapters.safetensors and adapters/0001200_adapters.safetensors.\n",
      "Iter 1210: Train loss 0.325, Learning Rate 1.000e-05, It/sec 2.667, Tokens/sec 1233.958, Trained Tokens 583254, Peak mem 4.490 GB\n",
      "Iter 1220: Train loss 0.415, Learning Rate 1.000e-05, It/sec 2.238, Tokens/sec 1223.008, Trained Tokens 588718, Peak mem 4.490 GB\n",
      "Iter 1230: Train loss 0.441, Learning Rate 1.000e-05, It/sec 2.438, Tokens/sec 1237.914, Trained Tokens 593795, Peak mem 4.490 GB\n",
      "Iter 1240: Train loss 0.451, Learning Rate 1.000e-05, It/sec 2.428, Tokens/sec 1275.805, Trained Tokens 599049, Peak mem 4.490 GB\n",
      "Iter 1250: Val loss 0.370, Val took 5.061s\n",
      "Iter 1250: Train loss 0.384, Learning Rate 1.000e-05, It/sec 2.650, Tokens/sec 1308.591, Trained Tokens 603988, Peak mem 4.490 GB\n",
      "Iter 1260: Train loss 0.387, Learning Rate 1.000e-05, It/sec 2.606, Tokens/sec 1288.917, Trained Tokens 608934, Peak mem 4.490 GB\n",
      "Iter 1270: Train loss 0.385, Learning Rate 1.000e-05, It/sec 2.645, Tokens/sec 1304.228, Trained Tokens 613865, Peak mem 4.490 GB\n",
      "Iter 1280: Train loss 0.396, Learning Rate 1.000e-05, It/sec 2.688, Tokens/sec 1355.160, Trained Tokens 618906, Peak mem 4.490 GB\n",
      "Iter 1290: Train loss 0.324, Learning Rate 1.000e-05, It/sec 3.176, Tokens/sec 1328.129, Trained Tokens 623088, Peak mem 4.490 GB\n",
      "Iter 1300: Val loss 0.331, Val took 4.499s\n",
      "Iter 1300: Train loss 0.378, Learning Rate 1.000e-05, It/sec 2.554, Tokens/sec 1303.767, Trained Tokens 628193, Peak mem 4.490 GB\n",
      "Iter 1300: Saved adapter weights to adapters/adapters.safetensors and adapters/0001300_adapters.safetensors.\n",
      "Iter 1310: Train loss 0.374, Learning Rate 1.000e-05, It/sec 2.732, Tokens/sec 1300.985, Trained Tokens 632955, Peak mem 4.490 GB\n",
      "Iter 1320: Train loss 0.385, Learning Rate 1.000e-05, It/sec 2.528, Tokens/sec 1237.122, Trained Tokens 637848, Peak mem 4.490 GB\n",
      "Iter 1330: Train loss 0.289, Learning Rate 1.000e-05, It/sec 2.876, Tokens/sec 1212.283, Trained Tokens 642063, Peak mem 4.490 GB\n",
      "Iter 1340: Train loss 0.413, Learning Rate 1.000e-05, It/sec 2.098, Tokens/sec 1135.997, Trained Tokens 647478, Peak mem 4.490 GB\n",
      "Iter 1350: Val loss 0.273, Val took 4.969s\n",
      "Iter 1350: Train loss 0.472, Learning Rate 1.000e-05, It/sec 2.113, Tokens/sec 1157.623, Trained Tokens 652957, Peak mem 4.490 GB\n",
      "Iter 1360: Train loss 0.409, Learning Rate 1.000e-05, It/sec 2.502, Tokens/sec 1248.666, Trained Tokens 657948, Peak mem 4.490 GB\n",
      "Iter 1370: Train loss 0.378, Learning Rate 1.000e-05, It/sec 2.631, Tokens/sec 1269.770, Trained Tokens 662774, Peak mem 4.490 GB\n",
      "Iter 1380: Train loss 0.328, Learning Rate 1.000e-05, It/sec 2.622, Tokens/sec 1259.393, Trained Tokens 667577, Peak mem 4.490 GB\n",
      "Iter 1390: Train loss 0.362, Learning Rate 1.000e-05, It/sec 2.648, Tokens/sec 1234.466, Trained Tokens 672239, Peak mem 4.490 GB\n",
      "Iter 1400: Val loss 0.306, Val took 4.989s\n",
      "Iter 1400: Train loss 0.404, Learning Rate 1.000e-05, It/sec 2.630, Tokens/sec 1261.393, Trained Tokens 677035, Peak mem 4.490 GB\n",
      "Iter 1400: Saved adapter weights to adapters/adapters.safetensors and adapters/0001400_adapters.safetensors.\n",
      "Iter 1410: Train loss 0.305, Learning Rate 1.000e-05, It/sec 2.800, Tokens/sec 1258.472, Trained Tokens 681530, Peak mem 4.490 GB\n",
      "Iter 1420: Train loss 0.365, Learning Rate 1.000e-05, It/sec 2.664, Tokens/sec 1241.518, Trained Tokens 686190, Peak mem 4.490 GB\n",
      "Iter 1430: Train loss 0.317, Learning Rate 1.000e-05, It/sec 2.913, Tokens/sec 1276.611, Trained Tokens 690572, Peak mem 4.490 GB\n",
      "Iter 1440: Train loss 0.361, Learning Rate 1.000e-05, It/sec 2.768, Tokens/sec 1263.721, Trained Tokens 695138, Peak mem 4.490 GB\n",
      "Iter 1450: Val loss 0.236, Val took 4.617s\n",
      "Iter 1450: Train loss 0.371, Learning Rate 1.000e-05, It/sec 2.661, Tokens/sec 1229.436, Trained Tokens 699758, Peak mem 4.490 GB\n",
      "Iter 1460: Train loss 0.345, Learning Rate 1.000e-05, It/sec 2.598, Tokens/sec 1196.688, Trained Tokens 704365, Peak mem 4.490 GB\n",
      "Iter 1470: Train loss 0.291, Learning Rate 1.000e-05, It/sec 2.521, Tokens/sec 1165.299, Trained Tokens 708987, Peak mem 4.490 GB\n",
      "Iter 1480: Train loss 0.241, Learning Rate 1.000e-05, It/sec 2.678, Tokens/sec 1223.730, Trained Tokens 713557, Peak mem 4.490 GB\n",
      "Iter 1490: Train loss 0.262, Learning Rate 1.000e-05, It/sec 2.716, Tokens/sec 1276.124, Trained Tokens 718255, Peak mem 4.490 GB\n",
      "Iter 1500: Val loss 0.214, Val took 5.110s\n",
      "Iter 1500: Train loss 0.283, Learning Rate 1.000e-05, It/sec 2.367, Tokens/sec 1244.938, Trained Tokens 723515, Peak mem 4.490 GB\n",
      "Iter 1500: Saved adapter weights to adapters/adapters.safetensors and adapters/0001500_adapters.safetensors.\n",
      "Iter 1510: Train loss 0.225, Learning Rate 1.000e-05, It/sec 2.528, Tokens/sec 1236.866, Trained Tokens 728407, Peak mem 4.490 GB\n",
      "Iter 1520: Train loss 0.220, Learning Rate 1.000e-05, It/sec 2.663, Tokens/sec 1227.526, Trained Tokens 733016, Peak mem 4.490 GB\n",
      "Iter 1530: Train loss 0.358, Learning Rate 1.000e-05, It/sec 2.154, Tokens/sec 1207.258, Trained Tokens 738621, Peak mem 4.490 GB\n",
      "Iter 1540: Train loss 0.208, Learning Rate 1.000e-05, It/sec 3.169, Tokens/sec 1289.449, Trained Tokens 742690, Peak mem 4.490 GB\n",
      "Iter 1550: Val loss 0.298, Val took 5.024s\n",
      "Iter 1550: Train loss 0.292, Learning Rate 1.000e-05, It/sec 2.496, Tokens/sec 1268.502, Trained Tokens 747773, Peak mem 4.490 GB\n",
      "Iter 1560: Train loss 0.276, Learning Rate 1.000e-05, It/sec 2.625, Tokens/sec 1265.330, Trained Tokens 752594, Peak mem 4.490 GB\n",
      "Iter 1570: Train loss 0.237, Learning Rate 1.000e-05, It/sec 2.750, Tokens/sec 1268.206, Trained Tokens 757206, Peak mem 4.490 GB\n",
      "Iter 1580: Train loss 0.268, Learning Rate 1.000e-05, It/sec 2.562, Tokens/sec 1264.105, Trained Tokens 762140, Peak mem 4.490 GB\n",
      "Iter 1590: Train loss 0.223, Learning Rate 1.000e-05, It/sec 2.994, Tokens/sec 1260.885, Trained Tokens 766352, Peak mem 4.490 GB\n",
      "Iter 1600: Val loss 0.254, Val took 5.146s\n",
      "Iter 1600: Train loss 0.225, Learning Rate 1.000e-05, It/sec 2.824, Tokens/sec 1249.983, Trained Tokens 770779, Peak mem 4.490 GB\n",
      "Iter 1600: Saved adapter weights to adapters/adapters.safetensors and adapters/0001600_adapters.safetensors.\n",
      "Iter 1610: Train loss 0.277, Learning Rate 1.000e-05, It/sec 2.370, Tokens/sec 1229.283, Trained Tokens 775965, Peak mem 4.490 GB\n",
      "Iter 1620: Train loss 0.259, Learning Rate 1.000e-05, It/sec 2.725, Tokens/sec 1282.620, Trained Tokens 780671, Peak mem 4.490 GB\n",
      "Iter 1630: Train loss 0.265, Learning Rate 1.000e-05, It/sec 2.486, Tokens/sec 1250.211, Trained Tokens 785701, Peak mem 4.490 GB\n",
      "Iter 1640: Train loss 0.284, Learning Rate 1.000e-05, It/sec 2.687, Tokens/sec 1266.165, Trained Tokens 790414, Peak mem 4.490 GB\n",
      "Iter 1650: Val loss 0.206, Val took 4.898s\n",
      "Iter 1650: Train loss 0.346, Learning Rate 1.000e-05, It/sec 2.267, Tokens/sec 1234.632, Trained Tokens 795860, Peak mem 4.490 GB\n",
      "Iter 1660: Train loss 0.228, Learning Rate 1.000e-05, It/sec 2.770, Tokens/sec 1272.162, Trained Tokens 800453, Peak mem 4.490 GB\n",
      "Iter 1670: Train loss 0.318, Learning Rate 1.000e-05, It/sec 2.479, Tokens/sec 1232.032, Trained Tokens 805423, Peak mem 4.490 GB\n",
      "Iter 1680: Train loss 0.313, Learning Rate 1.000e-05, It/sec 2.474, Tokens/sec 1226.126, Trained Tokens 810380, Peak mem 4.490 GB\n",
      "Iter 1690: Train loss 0.274, Learning Rate 1.000e-05, It/sec 2.625, Tokens/sec 1246.020, Trained Tokens 815127, Peak mem 4.490 GB\n",
      "Iter 1700: Val loss 0.262, Val took 5.314s\n",
      "Iter 1700: Train loss 0.276, Learning Rate 1.000e-05, It/sec 2.555, Tokens/sec 1270.790, Trained Tokens 820101, Peak mem 4.490 GB\n",
      "Iter 1700: Saved adapter weights to adapters/adapters.safetensors and adapters/0001700_adapters.safetensors.\n",
      "Iter 1710: Train loss 0.305, Learning Rate 1.000e-05, It/sec 2.548, Tokens/sec 1261.385, Trained Tokens 825051, Peak mem 4.490 GB\n",
      "Iter 1720: Train loss 0.258, Learning Rate 1.000e-05, It/sec 2.768, Tokens/sec 1252.413, Trained Tokens 829576, Peak mem 4.490 GB\n",
      "Iter 1730: Train loss 0.283, Learning Rate 1.000e-05, It/sec 2.552, Tokens/sec 1251.572, Trained Tokens 834481, Peak mem 4.490 GB\n",
      "Iter 1740: Train loss 0.284, Learning Rate 1.000e-05, It/sec 2.588, Tokens/sec 1263.409, Trained Tokens 839363, Peak mem 4.490 GB\n",
      "Iter 1750: Val loss 0.176, Val took 4.843s\n",
      "Iter 1750: Train loss 0.289, Learning Rate 1.000e-05, It/sec 2.316, Tokens/sec 1228.973, Trained Tokens 844670, Peak mem 4.490 GB\n",
      "Iter 1760: Train loss 0.239, Learning Rate 1.000e-05, It/sec 2.748, Tokens/sec 1286.238, Trained Tokens 849351, Peak mem 4.490 GB\n",
      "Iter 1770: Train loss 0.214, Learning Rate 1.000e-05, It/sec 2.353, Tokens/sec 1235.163, Trained Tokens 854601, Peak mem 4.490 GB\n",
      "Iter 1780: Train loss 0.187, Learning Rate 1.000e-05, It/sec 2.617, Tokens/sec 1255.956, Trained Tokens 859400, Peak mem 4.490 GB\n",
      "Iter 1790: Train loss 0.152, Learning Rate 1.000e-05, It/sec 2.669, Tokens/sec 1224.331, Trained Tokens 863988, Peak mem 4.490 GB\n",
      "Iter 1800: Val loss 0.199, Val took 5.238s\n",
      "Iter 1800: Train loss 0.133, Learning Rate 1.000e-05, It/sec 2.561, Tokens/sec 1220.289, Trained Tokens 868753, Peak mem 4.490 GB\n",
      "Iter 1800: Saved adapter weights to adapters/adapters.safetensors and adapters/0001800_adapters.safetensors.\n",
      "Iter 1810: Train loss 0.147, Learning Rate 1.000e-05, It/sec 2.855, Tokens/sec 1298.959, Trained Tokens 873302, Peak mem 4.490 GB\n",
      "Iter 1820: Train loss 0.205, Learning Rate 1.000e-05, It/sec 2.404, Tokens/sec 1267.414, Trained Tokens 878575, Peak mem 4.490 GB\n",
      "Iter 1830: Train loss 0.149, Learning Rate 1.000e-05, It/sec 2.749, Tokens/sec 1240.320, Trained Tokens 883087, Peak mem 4.490 GB\n",
      "Iter 1840: Train loss 0.181, Learning Rate 1.000e-05, It/sec 2.483, Tokens/sec 1268.899, Trained Tokens 888197, Peak mem 4.490 GB\n",
      "Iter 1850: Val loss 0.157, Val took 4.882s\n",
      "Iter 1850: Train loss 0.187, Learning Rate 1.000e-05, It/sec 2.484, Tokens/sec 1266.875, Trained Tokens 893298, Peak mem 4.490 GB\n",
      "Iter 1860: Train loss 0.223, Learning Rate 1.000e-05, It/sec 2.319, Tokens/sec 1251.340, Trained Tokens 898695, Peak mem 4.490 GB\n",
      "Iter 1870: Train loss 0.195, Learning Rate 1.000e-05, It/sec 2.531, Tokens/sec 1253.978, Trained Tokens 903650, Peak mem 4.490 GB\n",
      "Iter 1880: Train loss 0.245, Learning Rate 1.000e-05, It/sec 2.306, Tokens/sec 1272.845, Trained Tokens 909169, Peak mem 4.490 GB\n",
      "Iter 1890: Train loss 0.232, Learning Rate 1.000e-05, It/sec 2.463, Tokens/sec 1275.416, Trained Tokens 914348, Peak mem 4.490 GB\n",
      "Iter 1900: Val loss 0.206, Val took 5.463s\n",
      "Iter 1900: Train loss 0.164, Learning Rate 1.000e-05, It/sec 2.872, Tokens/sec 1259.262, Trained Tokens 918733, Peak mem 4.490 GB\n",
      "Iter 1900: Saved adapter weights to adapters/adapters.safetensors and adapters/0001900_adapters.safetensors.\n",
      "Iter 1910: Train loss 0.235, Learning Rate 1.000e-05, It/sec 2.515, Tokens/sec 1254.422, Trained Tokens 923721, Peak mem 4.490 GB\n",
      "Iter 1920: Train loss 0.182, Learning Rate 1.000e-05, It/sec 2.503, Tokens/sec 1237.383, Trained Tokens 928664, Peak mem 4.490 GB\n",
      "Iter 1930: Train loss 0.176, Learning Rate 1.000e-05, It/sec 2.723, Tokens/sec 1234.169, Trained Tokens 933196, Peak mem 4.490 GB\n",
      "Iter 1940: Train loss 0.174, Learning Rate 1.000e-05, It/sec 2.748, Tokens/sec 1241.200, Trained Tokens 937713, Peak mem 4.490 GB\n",
      "Iter 1950: Val loss 0.169, Val took 5.450s\n",
      "Iter 1950: Train loss 0.160, Learning Rate 1.000e-05, It/sec 2.874, Tokens/sec 1234.584, Trained Tokens 942009, Peak mem 4.490 GB\n",
      "Iter 1960: Train loss 0.216, Learning Rate 1.000e-05, It/sec 2.436, Tokens/sec 1244.543, Trained Tokens 947117, Peak mem 4.490 GB\n",
      "Iter 1970: Train loss 0.172, Learning Rate 1.000e-05, It/sec 2.567, Tokens/sec 1214.646, Trained Tokens 951849, Peak mem 4.490 GB\n",
      "Iter 1980: Train loss 0.195, Learning Rate 1.000e-05, It/sec 2.568, Tokens/sec 1236.145, Trained Tokens 956662, Peak mem 4.490 GB\n",
      "Iter 1990: Train loss 0.183, Learning Rate 1.000e-05, It/sec 2.847, Tokens/sec 1278.210, Trained Tokens 961151, Peak mem 4.490 GB\n",
      "Iter 2000: Val loss 0.151, Val took 5.066s\n",
      "Iter 2000: Train loss 0.155, Learning Rate 1.000e-05, It/sec 2.834, Tokens/sec 1239.272, Trained Tokens 965524, Peak mem 4.490 GB\n",
      "Iter 2000: Saved adapter weights to adapters/adapters.safetensors and adapters/0002000_adapters.safetensors.\n",
      "Iter 2010: Train loss 0.222, Learning Rate 1.000e-05, It/sec 2.471, Tokens/sec 1242.664, Trained Tokens 970552, Peak mem 4.490 GB\n",
      "Iter 2020: Train loss 0.171, Learning Rate 1.000e-05, It/sec 2.837, Tokens/sec 1248.415, Trained Tokens 974952, Peak mem 4.490 GB\n",
      "Iter 2030: Train loss 0.191, Learning Rate 1.000e-05, It/sec 2.690, Tokens/sec 1250.736, Trained Tokens 979602, Peak mem 4.490 GB\n",
      "Iter 2040: Train loss 0.203, Learning Rate 1.000e-05, It/sec 2.690, Tokens/sec 1276.346, Trained Tokens 984346, Peak mem 4.490 GB\n",
      "Iter 2050: Val loss 0.156, Val took 5.215s\n",
      "Iter 2050: Train loss 0.208, Learning Rate 1.000e-05, It/sec 2.564, Tokens/sec 1257.047, Trained Tokens 989248, Peak mem 4.490 GB\n",
      "Iter 2060: Train loss 0.142, Learning Rate 1.000e-05, It/sec 2.531, Tokens/sec 1284.494, Trained Tokens 994324, Peak mem 4.490 GB\n",
      "Iter 2070: Train loss 0.114, Learning Rate 1.000e-05, It/sec 2.651, Tokens/sec 1257.689, Trained Tokens 999068, Peak mem 4.490 GB\n",
      "Iter 2080: Train loss 0.114, Learning Rate 1.000e-05, It/sec 2.493, Tokens/sec 1234.785, Trained Tokens 1004022, Peak mem 4.490 GB\n",
      "Iter 2090: Train loss 0.110, Learning Rate 1.000e-05, It/sec 2.821, Tokens/sec 1264.445, Trained Tokens 1008505, Peak mem 4.490 GB\n",
      "Iter 2100: Val loss 0.142, Val took 5.202s\n",
      "Iter 2100: Train loss 0.130, Learning Rate 1.000e-05, It/sec 2.764, Tokens/sec 1258.274, Trained Tokens 1013057, Peak mem 4.490 GB\n",
      "Iter 2100: Saved adapter weights to adapters/adapters.safetensors and adapters/0002100_adapters.safetensors.\n",
      "Iter 2110: Train loss 0.118, Learning Rate 1.000e-05, It/sec 2.549, Tokens/sec 1245.568, Trained Tokens 1017944, Peak mem 4.490 GB\n",
      "Iter 2120: Train loss 0.105, Learning Rate 1.000e-05, It/sec 2.916, Tokens/sec 1257.154, Trained Tokens 1022255, Peak mem 4.490 GB\n",
      "Iter 2130: Train loss 0.144, Learning Rate 1.000e-05, It/sec 2.500, Tokens/sec 1259.655, Trained Tokens 1027294, Peak mem 4.490 GB\n",
      "Iter 2140: Train loss 0.108, Learning Rate 1.000e-05, It/sec 2.732, Tokens/sec 1232.500, Trained Tokens 1031806, Peak mem 4.490 GB\n",
      "Iter 2150: Val loss 0.113, Val took 5.064s\n",
      "Iter 2150: Train loss 0.191, Learning Rate 1.000e-05, It/sec 2.159, Tokens/sec 1242.757, Trained Tokens 1037561, Peak mem 4.490 GB\n",
      "Iter 2160: Train loss 0.135, Learning Rate 1.000e-05, It/sec 2.664, Tokens/sec 1261.751, Trained Tokens 1042298, Peak mem 4.490 GB\n",
      "Iter 2170: Train loss 0.113, Learning Rate 1.000e-05, It/sec 2.802, Tokens/sec 1262.993, Trained Tokens 1046805, Peak mem 4.490 GB\n",
      "Iter 2180: Train loss 0.152, Learning Rate 1.000e-05, It/sec 2.345, Tokens/sec 1266.069, Trained Tokens 1052203, Peak mem 4.490 GB\n",
      "Iter 2190: Train loss 0.110, Learning Rate 1.000e-05, It/sec 2.967, Tokens/sec 1297.850, Trained Tokens 1056577, Peak mem 4.490 GB\n",
      "Iter 2200: Val loss 0.082, Val took 4.695s\n",
      "Iter 2200: Train loss 0.160, Learning Rate 1.000e-05, It/sec 2.363, Tokens/sec 1259.542, Trained Tokens 1061907, Peak mem 4.490 GB\n",
      "Iter 2200: Saved adapter weights to adapters/adapters.safetensors and adapters/0002200_adapters.safetensors.\n",
      "Iter 2210: Train loss 0.147, Learning Rate 1.000e-05, It/sec 2.533, Tokens/sec 1272.649, Trained Tokens 1066931, Peak mem 4.490 GB\n",
      "Iter 2220: Train loss 0.145, Learning Rate 1.000e-05, It/sec 2.331, Tokens/sec 1250.189, Trained Tokens 1072295, Peak mem 4.490 GB\n",
      "Iter 2230: Train loss 0.127, Learning Rate 1.000e-05, It/sec 2.510, Tokens/sec 1246.063, Trained Tokens 1077259, Peak mem 4.490 GB\n",
      "Iter 2240: Train loss 0.108, Learning Rate 1.000e-05, It/sec 2.790, Tokens/sec 1240.658, Trained Tokens 1081706, Peak mem 4.490 GB\n",
      "Iter 2250: Val loss 0.115, Val took 5.160s\n",
      "Iter 2250: Train loss 0.131, Learning Rate 1.000e-05, It/sec 2.873, Tokens/sec 1263.063, Trained Tokens 1086102, Peak mem 4.490 GB\n",
      "Iter 2260: Train loss 0.167, Learning Rate 1.000e-05, It/sec 2.280, Tokens/sec 1216.193, Trained Tokens 1091437, Peak mem 4.490 GB\n",
      "Iter 2270: Train loss 0.105, Learning Rate 1.000e-05, It/sec 2.995, Tokens/sec 1241.742, Trained Tokens 1095583, Peak mem 4.490 GB\n",
      "Iter 2280: Train loss 0.156, Learning Rate 1.000e-05, It/sec 2.480, Tokens/sec 1255.098, Trained Tokens 1100643, Peak mem 4.490 GB\n",
      "Iter 2290: Train loss 0.157, Learning Rate 1.000e-05, It/sec 2.517, Tokens/sec 1240.865, Trained Tokens 1105573, Peak mem 4.490 GB\n",
      "Iter 2300: Val loss 0.112, Val took 5.125s\n",
      "Iter 2300: Train loss 0.126, Learning Rate 1.000e-05, It/sec 2.804, Tokens/sec 1251.420, Trained Tokens 1110036, Peak mem 4.490 GB\n",
      "Iter 2300: Saved adapter weights to adapters/adapters.safetensors and adapters/0002300_adapters.safetensors.\n",
      "Iter 2310: Train loss 0.140, Learning Rate 1.000e-05, It/sec 2.481, Tokens/sec 1241.130, Trained Tokens 1115038, Peak mem 4.490 GB\n",
      "Iter 2320: Train loss 0.156, Learning Rate 1.000e-05, It/sec 2.546, Tokens/sec 1252.828, Trained Tokens 1119959, Peak mem 4.490 GB\n",
      "Iter 2330: Train loss 0.133, Learning Rate 1.000e-05, It/sec 2.834, Tokens/sec 1263.566, Trained Tokens 1124418, Peak mem 4.490 GB\n",
      "Iter 2340: Train loss 0.143, Learning Rate 1.000e-05, It/sec 2.494, Tokens/sec 1278.373, Trained Tokens 1129543, Peak mem 4.490 GB\n",
      "Iter 2350: Val loss 0.108, Val took 5.035s\n",
      "Iter 2350: Train loss 0.089, Learning Rate 1.000e-05, It/sec 2.893, Tokens/sec 1227.869, Trained Tokens 1133788, Peak mem 4.490 GB\n",
      "Iter 2360: Train loss 0.074, Learning Rate 1.000e-05, It/sec 2.953, Tokens/sec 1249.609, Trained Tokens 1138020, Peak mem 4.490 GB\n",
      "Iter 2370: Train loss 0.145, Learning Rate 1.000e-05, It/sec 1.968, Tokens/sec 1211.084, Trained Tokens 1144175, Peak mem 4.490 GB\n",
      "Iter 2380: Train loss 0.091, Learning Rate 1.000e-05, It/sec 2.397, Tokens/sec 1255.306, Trained Tokens 1149412, Peak mem 4.490 GB\n",
      "Iter 2390: Train loss 0.075, Learning Rate 1.000e-05, It/sec 2.915, Tokens/sec 1238.532, Trained Tokens 1153661, Peak mem 4.490 GB\n",
      "Iter 2400: Val loss 0.099, Val took 4.989s\n",
      "Iter 2400: Train loss 0.080, Learning Rate 1.000e-05, It/sec 2.667, Tokens/sec 1256.801, Trained Tokens 1158374, Peak mem 4.490 GB\n",
      "Iter 2400: Saved adapter weights to adapters/adapters.safetensors and adapters/0002400_adapters.safetensors.\n",
      "Iter 2410: Train loss 0.089, Learning Rate 1.000e-05, It/sec 2.639, Tokens/sec 1262.805, Trained Tokens 1163160, Peak mem 4.490 GB\n",
      "Iter 2420: Train loss 0.079, Learning Rate 1.000e-05, It/sec 2.557, Tokens/sec 1232.258, Trained Tokens 1167979, Peak mem 4.490 GB\n",
      "Iter 2430: Train loss 0.084, Learning Rate 1.000e-05, It/sec 2.617, Tokens/sec 1275.072, Trained Tokens 1172851, Peak mem 4.490 GB\n",
      "Iter 2440: Train loss 0.106, Learning Rate 1.000e-05, It/sec 2.347, Tokens/sec 1251.712, Trained Tokens 1178184, Peak mem 4.490 GB\n",
      "Iter 2450: Val loss 0.068, Val took 4.851s\n",
      "Iter 2450: Train loss 0.082, Learning Rate 1.000e-05, It/sec 2.659, Tokens/sec 1256.590, Trained Tokens 1182909, Peak mem 4.490 GB\n",
      "Iter 2460: Train loss 0.068, Learning Rate 1.000e-05, It/sec 2.649, Tokens/sec 1234.889, Trained Tokens 1187570, Peak mem 4.490 GB\n",
      "Iter 2470: Train loss 0.079, Learning Rate 1.000e-05, It/sec 2.837, Tokens/sec 1250.005, Trained Tokens 1191976, Peak mem 4.490 GB\n",
      "Iter 2480: Train loss 0.097, Learning Rate 1.000e-05, It/sec 2.687, Tokens/sec 1280.586, Trained Tokens 1196742, Peak mem 4.490 GB\n",
      "Iter 2490: Train loss 0.097, Learning Rate 1.000e-05, It/sec 2.434, Tokens/sec 1256.985, Trained Tokens 1201906, Peak mem 4.490 GB\n",
      "Iter 2500: Val loss 0.076, Val took 5.241s\n",
      "Iter 2500: Train loss 0.094, Learning Rate 1.000e-05, It/sec 2.625, Tokens/sec 1265.150, Trained Tokens 1206725, Peak mem 4.490 GB\n",
      "Iter 2500: Saved adapter weights to adapters/adapters.safetensors and adapters/0002500_adapters.safetensors.\n",
      "Iter 2510: Train loss 0.112, Learning Rate 1.000e-05, It/sec 2.475, Tokens/sec 1237.580, Trained Tokens 1211725, Peak mem 4.490 GB\n",
      "Iter 2520: Train loss 0.092, Learning Rate 1.000e-05, It/sec 2.814, Tokens/sec 1286.468, Trained Tokens 1216297, Peak mem 4.490 GB\n",
      "Iter 2530: Train loss 0.097, Learning Rate 1.000e-05, It/sec 2.637, Tokens/sec 1274.619, Trained Tokens 1221131, Peak mem 4.490 GB\n",
      "Iter 2540: Train loss 0.110, Learning Rate 1.000e-05, It/sec 2.495, Tokens/sec 1274.792, Trained Tokens 1226240, Peak mem 4.490 GB\n",
      "Iter 2550: Val loss 0.069, Val took 4.800s\n",
      "Iter 2550: Train loss 0.087, Learning Rate 1.000e-05, It/sec 2.966, Tokens/sec 1276.110, Trained Tokens 1230543, Peak mem 4.490 GB\n",
      "Iter 2560: Train loss 0.118, Learning Rate 1.000e-05, It/sec 2.688, Tokens/sec 1250.626, Trained Tokens 1235195, Peak mem 4.490 GB\n",
      "Iter 2570: Train loss 0.109, Learning Rate 1.000e-05, It/sec 2.514, Tokens/sec 1239.524, Trained Tokens 1240125, Peak mem 4.490 GB\n",
      "Iter 2580: Train loss 0.123, Learning Rate 1.000e-05, It/sec 2.293, Tokens/sec 1238.890, Trained Tokens 1245528, Peak mem 4.490 GB\n",
      "Iter 2590: Train loss 0.105, Learning Rate 1.000e-05, It/sec 2.621, Tokens/sec 1247.756, Trained Tokens 1250288, Peak mem 4.490 GB\n",
      "Iter 2600: Val loss 0.072, Val took 4.955s\n",
      "Iter 2600: Train loss 0.086, Learning Rate 1.000e-05, It/sec 2.894, Tokens/sec 1267.397, Trained Tokens 1254667, Peak mem 4.490 GB\n",
      "Iter 2600: Saved adapter weights to adapters/adapters.safetensors and adapters/0002600_adapters.safetensors.\n",
      "Iter 2610: Train loss 0.122, Learning Rate 1.000e-05, It/sec 2.426, Tokens/sec 1240.620, Trained Tokens 1259781, Peak mem 4.490 GB\n",
      "Iter 2620: Train loss 0.095, Learning Rate 1.000e-05, It/sec 2.687, Tokens/sec 1259.745, Trained Tokens 1264470, Peak mem 4.490 GB\n",
      "Iter 2630: Train loss 0.109, Learning Rate 1.000e-05, It/sec 2.444, Tokens/sec 1264.435, Trained Tokens 1269644, Peak mem 4.490 GB\n",
      "Iter 2640: Train loss 0.072, Learning Rate 1.000e-05, It/sec 3.095, Tokens/sec 1303.814, Trained Tokens 1273856, Peak mem 4.490 GB\n",
      "Iter 2650: Val loss 0.064, Val took 4.668s\n",
      "Iter 2650: Train loss 0.075, Learning Rate 1.000e-05, It/sec 2.728, Tokens/sec 1281.580, Trained Tokens 1278554, Peak mem 4.490 GB\n",
      "Iter 2660: Train loss 0.092, Learning Rate 1.000e-05, It/sec 2.161, Tokens/sec 1268.589, Trained Tokens 1284425, Peak mem 4.490 GB\n",
      "Iter 2670: Train loss 0.054, Learning Rate 1.000e-05, It/sec 2.744, Tokens/sec 1287.804, Trained Tokens 1289118, Peak mem 4.490 GB\n",
      "Iter 2680: Train loss 0.070, Learning Rate 1.000e-05, It/sec 2.376, Tokens/sec 1262.728, Trained Tokens 1294433, Peak mem 4.490 GB\n",
      "Iter 2690: Train loss 0.069, Learning Rate 1.000e-05, It/sec 2.677, Tokens/sec 1313.309, Trained Tokens 1299338, Peak mem 4.490 GB\n",
      "Iter 2700: Val loss 0.060, Val took 4.834s\n",
      "Iter 2700: Train loss 0.067, Learning Rate 1.000e-05, It/sec 2.654, Tokens/sec 1281.540, Trained Tokens 1304166, Peak mem 4.490 GB\n",
      "Iter 2700: Saved adapter weights to adapters/adapters.safetensors and adapters/0002700_adapters.safetensors.\n",
      "Iter 2710: Train loss 0.070, Learning Rate 1.000e-05, It/sec 2.851, Tokens/sec 1311.610, Trained Tokens 1308767, Peak mem 4.490 GB\n",
      "Iter 2720: Train loss 0.062, Learning Rate 1.000e-05, It/sec 2.874, Tokens/sec 1271.704, Trained Tokens 1313192, Peak mem 4.490 GB\n",
      "Iter 2730: Train loss 0.060, Learning Rate 1.000e-05, It/sec 2.915, Tokens/sec 1272.309, Trained Tokens 1317556, Peak mem 4.490 GB\n",
      "Iter 2740: Train loss 0.082, Learning Rate 1.000e-05, It/sec 2.491, Tokens/sec 1315.250, Trained Tokens 1322835, Peak mem 4.490 GB\n",
      "Iter 2750: Val loss 0.058, Val took 4.674s\n",
      "Iter 2750: Train loss 0.067, Learning Rate 1.000e-05, It/sec 2.896, Tokens/sec 1281.331, Trained Tokens 1327259, Peak mem 4.490 GB\n",
      "Iter 2760: Train loss 0.093, Learning Rate 1.000e-05, It/sec 2.358, Tokens/sec 1257.744, Trained Tokens 1332594, Peak mem 4.490 GB\n",
      "Iter 2770: Train loss 0.071, Learning Rate 1.000e-05, It/sec 2.810, Tokens/sec 1305.063, Trained Tokens 1337238, Peak mem 4.490 GB\n",
      "Iter 2780: Train loss 0.092, Learning Rate 1.000e-05, It/sec 2.356, Tokens/sec 1275.641, Trained Tokens 1342653, Peak mem 4.490 GB\n",
      "Iter 2790: Train loss 0.067, Learning Rate 1.000e-05, It/sec 2.640, Tokens/sec 1228.284, Trained Tokens 1347305, Peak mem 4.490 GB\n",
      "Iter 2800: Val loss 0.063, Val took 4.912s\n",
      "Iter 2800: Train loss 0.071, Learning Rate 1.000e-05, It/sec 2.667, Tokens/sec 1259.523, Trained Tokens 1352027, Peak mem 4.490 GB\n",
      "Iter 2800: Saved adapter weights to adapters/adapters.safetensors and adapters/0002800_adapters.safetensors.\n",
      "Iter 2810: Train loss 0.073, Learning Rate 1.000e-05, It/sec 2.637, Tokens/sec 1256.041, Trained Tokens 1356790, Peak mem 4.490 GB\n",
      "Iter 2820: Train loss 0.067, Learning Rate 1.000e-05, It/sec 2.688, Tokens/sec 1249.861, Trained Tokens 1361440, Peak mem 4.490 GB\n",
      "Iter 2830: Train loss 0.074, Learning Rate 1.000e-05, It/sec 2.702, Tokens/sec 1277.141, Trained Tokens 1366167, Peak mem 4.490 GB\n",
      "Iter 2840: Train loss 0.088, Learning Rate 1.000e-05, It/sec 2.278, Tokens/sec 1226.625, Trained Tokens 1371552, Peak mem 4.490 GB\n",
      "Iter 2850: Val loss 0.064, Val took 5.284s\n",
      "Iter 2850: Train loss 0.075, Learning Rate 1.000e-05, It/sec 2.579, Tokens/sec 1242.757, Trained Tokens 1376371, Peak mem 4.490 GB\n",
      "Iter 2860: Train loss 0.059, Learning Rate 1.000e-05, It/sec 3.010, Tokens/sec 1262.142, Trained Tokens 1380564, Peak mem 4.490 GB\n",
      "Iter 2870: Train loss 0.081, Learning Rate 1.000e-05, It/sec 2.806, Tokens/sec 1259.435, Trained Tokens 1385053, Peak mem 4.490 GB\n",
      "Iter 2880: Train loss 0.076, Learning Rate 1.000e-05, It/sec 2.557, Tokens/sec 1258.858, Trained Tokens 1389977, Peak mem 4.490 GB\n",
      "Iter 2890: Train loss 0.071, Learning Rate 1.000e-05, It/sec 2.980, Tokens/sec 1288.105, Trained Tokens 1394299, Peak mem 4.490 GB\n",
      "Iter 2900: Val loss 0.051, Val took 4.896s\n",
      "Iter 2900: Train loss 0.086, Learning Rate 1.000e-05, It/sec 2.237, Tokens/sec 1252.109, Trained Tokens 1399896, Peak mem 4.490 GB\n",
      "Iter 2900: Saved adapter weights to adapters/adapters.safetensors and adapters/0002900_adapters.safetensors.\n",
      "Iter 2910: Train loss 0.079, Learning Rate 1.000e-05, It/sec 2.640, Tokens/sec 1267.969, Trained Tokens 1404699, Peak mem 4.490 GB\n",
      "Iter 2920: Train loss 0.073, Learning Rate 1.000e-05, It/sec 2.726, Tokens/sec 1260.477, Trained Tokens 1409323, Peak mem 4.490 GB\n",
      "Iter 2930: Val loss 0.060, Val took 5.223s\n",
      "Iter 2930: Train loss 0.072, Learning Rate 1.000e-05, It/sec 2.814, Tokens/sec 1299.062, Trained Tokens 1413940, Peak mem 4.490 GB\n",
      "Saved final weights to adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import mlx.optimizers as optim\n",
    "from pathlib import Path\n",
    "from mlx_lm.tuner import TrainingArgs, datasets, linear_to_lora_layers, train\n",
    "from mlx.utils import tree_flatten\n",
    "\n",
    "dataset = dataset.map(\n",
    "    remove_columns=[\"Question\", \"Response\", \"Complex_CoT\"]\n",
    ")\n",
    "\n",
    "configs = {\n",
    "    \"mask_prompt\": False,\n",
    "    \"prompt_feature\": \"prompt\",\n",
    "    \"text_feature\": \"text\",\n",
    "    \"completion_feature\": \"completion\",\n",
    "    \"chat_feature\": \"messages\",\n",
    "}\n",
    "\n",
    "train_dataset = datasets.CacheDataset(datasets.create_dataset(\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    configs\n",
    "))\n",
    "\n",
    "opt = optim.Adam(learning_rate=1e-5)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = {\n",
    "    \"num_layers\": 16,\n",
    "    \"lora_parameters\": {\n",
    "    \"rank\": 32,                          # Rank of the LoRA update matrices\n",
    "    \"scale\": 64,                        # Scaling factor for LoRA\n",
    "    \"dropout\": 0.0                    # Dropout for regularization\n",
    "    }\n",
    "}\n",
    "\n",
    "adapter_path = Path(\"adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)\n",
    "\n",
    "_ = model.freeze()\n",
    "\n",
    "training_args = TrainingArgs(\n",
    "    iters=2930,\n",
    "    batch_size=1,\n",
    "    steps_per_eval=50,\n",
    "    adapter_file=adapter_path / \"adapters.safetensors\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")\n",
    "\n",
    "# Train the model\n",
    "gc.collect()\n",
    "\n",
    "_ = model.train() # Запуск режима обучения\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=train_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_lora, tokenizer = load(\"Qwen/Qwen3-0.6B\", adapter_path=\"adapters\")\n",
    "\n",
    "question = \"Which genres does Nageki no Kenkou Yuuryouji fall under?\"\n",
    "#question = \"What is anime?\"\n",
    "prompt = inference_prompt_style.format(question) # Используем правильный шаблон\n",
    "\n",
    "response = generate(model_lora, tokenizer, prompt=prompt, max_tokens=1200)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_anime_qwen3 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
