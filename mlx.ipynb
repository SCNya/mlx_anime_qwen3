{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fa358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 62072.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>\n",
      "Okay, so the user is asking how many members the anime \"Peach Girl\" has. Let me start by recalling what I know about this series. I remember that \"Peach Girl\" is a popular Japanese anime, and it's known for its unique characters and story. The main characters are the protagonist, a girl named Peach, and her friends. But wait, I think there are more characters involved. Let me think... Oh right, there's also the character of the girl who is the main character of the story, and then there's the other main character, maybe a friend or a sibling. Wait, I'm getting confused. Let me check my memory again.\n",
      "\n",
      "I think the main characters are Peach, her friend, and another girl. But I'm not entirely sure. Maybe there are more. Oh, right! The anime has a main protagonist, a main character, and a few other characters. Let me confirm. The original series has the main characters: Peach, her friend, and another girl. So, that's three main characters. But I'm not 100% sure. Maybe there are more. Alternatively, perhaps the answer is that it has three main characters. But I need to be careful here. Let me think again. The original series is called \"Peach Girl,\" and it's known for its three main characters: Peach, her friend, and another girl. So, the answer would be three. But I should make sure. Alternatively, maybe there are more. For example, the anime has a main protagonist, a main character, and a few others. Let me check my notes. Yes, I think the answer is three. So, the response should state that \"Peach Girl\" has three main members.\n",
      "</think>\n",
      "\n",
      "\"Peach Girl\" has three main members: the protagonist Peach, her friend, and another character who plays a central role in the story.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Загрузка модели и токенизатора с MLX\n",
    "model, tokenizer = load(\"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "# Шаблон для обучающих данных\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are an expert in Japanese animation with advanced knowledge in anime, manga, Japanese cartoons and animated films.\n",
    "Please answer the following anime fan question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\"\n",
    "\n",
    "# Функция для форматирования примеров\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    complex_cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for question, cot, response in zip(inputs, complex_cots, outputs):\n",
    "        # Добавляем EOS-токен к ответу, если его нет\n",
    "        if not response.endswith(tokenizer.eos_token):\n",
    "            response += tokenizer.eos_token\n",
    "        text = train_prompt_style.format(question, cot, response)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Загрузка и обработка датасета\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"datasets-anime-sharegpt-2025-05-24.json\",\n",
    "    split=\"train\"\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Шаблон для инференса\n",
    "inference_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are an expert in Japanese animation with advanced knowledge in anime, manga, Japanese cartoons and animated films.\n",
    "Please answer the following anime fan question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "\"\"\"\n",
    "\n",
    "# Выполнение инференса\n",
    "question = dataset[10]['Question']\n",
    "prompt = inference_prompt_style.format(question)\n",
    "\n",
    "outputs = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_tokens=1200\n",
    ")\n",
    "\n",
    "outputs = generate(model, tokenizer, prompt, max_tokens=1200)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042dd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c35f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe7263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 1146880\n",
      "Starting training..., iters: 300\n",
      "Iter 1: Val loss 2.437, Val took 5.296s\n",
      "Iter 10: Train loss 1.916, Learning Rate 1.000e-05, It/sec 2.631, Tokens/sec 1283.732, Trained Tokens 4879, Peak mem 3.836 GB\n",
      "Iter 20: Train loss 1.431, Learning Rate 1.000e-05, It/sec 3.055, Tokens/sec 1321.772, Trained Tokens 9205, Peak mem 3.836 GB\n",
      "Iter 30: Train loss 1.314, Learning Rate 1.000e-05, It/sec 2.447, Tokens/sec 1307.336, Trained Tokens 14548, Peak mem 4.231 GB\n",
      "Iter 40: Train loss 1.275, Learning Rate 1.000e-05, It/sec 2.938, Tokens/sec 1354.214, Trained Tokens 19158, Peak mem 4.231 GB\n",
      "Iter 50: Val loss 1.243, Val took 5.220s\n",
      "Iter 50: Train loss 1.081, Learning Rate 1.000e-05, It/sec 2.843, Tokens/sec 1317.300, Trained Tokens 23792, Peak mem 4.231 GB\n",
      "Iter 60: Train loss 1.067, Learning Rate 1.000e-05, It/sec 3.027, Tokens/sec 1356.673, Trained Tokens 28274, Peak mem 4.231 GB\n",
      "Iter 70: Train loss 1.203, Learning Rate 1.000e-05, It/sec 2.405, Tokens/sec 1294.374, Trained Tokens 33655, Peak mem 4.231 GB\n",
      "Iter 80: Train loss 1.162, Learning Rate 1.000e-05, It/sec 2.717, Tokens/sec 1309.688, Trained Tokens 38476, Peak mem 4.231 GB\n",
      "Iter 90: Train loss 1.124, Learning Rate 1.000e-05, It/sec 2.556, Tokens/sec 1333.975, Trained Tokens 43695, Peak mem 4.231 GB\n",
      "Iter 100: Val loss 1.138, Val took 5.059s\n",
      "Iter 100: Train loss 1.165, Learning Rate 1.000e-05, It/sec 2.814, Tokens/sec 1290.958, Trained Tokens 48282, Peak mem 4.231 GB\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 1.171, Learning Rate 1.000e-05, It/sec 2.677, Tokens/sec 1285.060, Trained Tokens 53083, Peak mem 4.231 GB\n",
      "Iter 120: Train loss 1.133, Learning Rate 1.000e-05, It/sec 2.719, Tokens/sec 1289.463, Trained Tokens 57826, Peak mem 4.231 GB\n",
      "Iter 130: Train loss 1.067, Learning Rate 1.000e-05, It/sec 2.803, Tokens/sec 1330.019, Trained Tokens 62571, Peak mem 4.231 GB\n",
      "Iter 140: Train loss 1.057, Learning Rate 1.000e-05, It/sec 2.900, Tokens/sec 1279.088, Trained Tokens 66982, Peak mem 4.231 GB\n",
      "Iter 150: Val loss 1.144, Val took 5.181s\n",
      "Iter 150: Train loss 0.955, Learning Rate 1.000e-05, It/sec 3.092, Tokens/sec 1329.097, Trained Tokens 71280, Peak mem 4.231 GB\n",
      "Iter 160: Train loss 1.052, Learning Rate 1.000e-05, It/sec 2.972, Tokens/sec 1308.470, Trained Tokens 75682, Peak mem 4.231 GB\n",
      "Iter 170: Train loss 1.195, Learning Rate 1.000e-05, It/sec 2.702, Tokens/sec 1327.693, Trained Tokens 80596, Peak mem 4.231 GB\n",
      "Iter 180: Train loss 1.117, Learning Rate 1.000e-05, It/sec 2.427, Tokens/sec 1321.532, Trained Tokens 86042, Peak mem 4.231 GB\n",
      "Iter 190: Train loss 1.083, Learning Rate 1.000e-05, It/sec 2.644, Tokens/sec 1314.006, Trained Tokens 91012, Peak mem 4.231 GB\n",
      "Iter 200: Val loss 1.089, Val took 5.310s\n",
      "Iter 200: Train loss 1.014, Learning Rate 1.000e-05, It/sec 2.847, Tokens/sec 1335.582, Trained Tokens 95703, Peak mem 4.231 GB\n",
      "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 1.093, Learning Rate 1.000e-05, It/sec 2.499, Tokens/sec 1311.940, Trained Tokens 100953, Peak mem 4.231 GB\n",
      "Iter 220: Train loss 1.147, Learning Rate 1.000e-05, It/sec 2.786, Tokens/sec 1331.910, Trained Tokens 105734, Peak mem 4.231 GB\n",
      "Iter 230: Train loss 1.041, Learning Rate 1.000e-05, It/sec 3.061, Tokens/sec 1312.204, Trained Tokens 110021, Peak mem 4.231 GB\n",
      "Iter 240: Train loss 1.075, Learning Rate 1.000e-05, It/sec 2.819, Tokens/sec 1309.845, Trained Tokens 114667, Peak mem 4.231 GB\n",
      "Iter 250: Val loss 1.115, Val took 5.261s\n",
      "Iter 250: Train loss 0.929, Learning Rate 1.000e-05, It/sec 2.942, Tokens/sec 1331.229, Trained Tokens 119192, Peak mem 4.231 GB\n",
      "Iter 260: Train loss 1.123, Learning Rate 1.000e-05, It/sec 2.401, Tokens/sec 1311.483, Trained Tokens 124655, Peak mem 4.231 GB\n",
      "Iter 270: Train loss 1.037, Learning Rate 1.000e-05, It/sec 2.750, Tokens/sec 1332.750, Trained Tokens 129501, Peak mem 4.231 GB\n",
      "Iter 280: Train loss 1.015, Learning Rate 1.000e-05, It/sec 2.634, Tokens/sec 1338.352, Trained Tokens 134582, Peak mem 4.231 GB\n",
      "Iter 290: Train loss 0.928, Learning Rate 1.000e-05, It/sec 2.901, Tokens/sec 1301.243, Trained Tokens 139068, Peak mem 4.231 GB\n",
      "Iter 300: Val loss 1.085, Val took 5.251s\n",
      "Iter 300: Train loss 0.866, Learning Rate 1.000e-05, It/sec 2.939, Tokens/sec 1301.714, Trained Tokens 143497, Peak mem 4.231 GB\n",
      "Iter 300: Saved adapter weights to adapters/adapters.safetensors and adapters/0000300_adapters.safetensors.\n",
      "Saved final weights to adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import mlx.optimizers as optim\n",
    "from pathlib import Path\n",
    "from mlx_lm.tuner import TrainingArgs, datasets, linear_to_lora_layers, train\n",
    "from mlx.utils import tree_flatten\n",
    "\n",
    "dataset = dataset.map(\n",
    "    remove_columns=[\"Question\", \"Response\", \"Complex_CoT\"]\n",
    ").train_test_split(0.1)\n",
    "\n",
    "configs = {\n",
    "    \"mask_prompt\": False,\n",
    "    \"prompt_feature\": \"prompt\",\n",
    "    \"text_feature\": \"text\",\n",
    "    \"completion_feature\": \"completion\",\n",
    "    \"chat_feature\": \"messages\",\n",
    "}\n",
    "\n",
    "train_dataset = datasets.CacheDataset(datasets.create_dataset(\n",
    "    dataset[\"train\"],\n",
    "    tokenizer,\n",
    "    configs\n",
    "))\n",
    "\n",
    "val_dataset = datasets.CacheDataset(datasets.create_dataset(\n",
    "    dataset[\"test\"],\n",
    "    tokenizer,\n",
    "    configs\n",
    "))\n",
    "\n",
    "opt = optim.Adam(learning_rate=1e-5)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = {\n",
    "    \"num_layers\": 14,\n",
    "    \"lora_parameters\": {\n",
    "    \"rank\": 16,                          # Rank of the LoRA update matrices\n",
    "    \"scale\": 32,                        # Scaling factor for LoRA\n",
    "    \"dropout\": 0.0                    # Dropout for regularization\n",
    "    }\n",
    "}\n",
    "\n",
    "adapter_path = Path(\"adapters\")\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "with open(adapter_path / \"adapter_config.json\", \"w\") as fid:\n",
    "    json.dump(lora_config, fid, indent=4)\n",
    "\n",
    "_ = model.freeze()\n",
    "\n",
    "training_args = TrainingArgs(\n",
    "    iters=300,\n",
    "    batch_size=1,\n",
    "    steps_per_eval=50,\n",
    "    adapter_file=adapter_path / \"adapters.safetensors\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])\n",
    "\n",
    "num_train_params = (\n",
    "    sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    ")\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")\n",
    "\n",
    "# Train the model\n",
    "gc.collect()\n",
    "\n",
    "_ = model.train() # Запуск режима обучения\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    optimizer=opt,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a4c166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 66126.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Optimized Chain of Thought:**  \n",
      "1. **Identify Key Information:** The question asks which genres Nageki no Kenkou Yuuryouji falls under.  \n",
      "2. **Analyze Content:** The provided reference data includes a \"Genres\" field, which lists \"Shoujo,\" \"Romance,\" and \"Comedy.\"  \n",
      "3. **Confirm Consistency:** No conflicting genres are mentioned elsewhere in the data.  \n",
      "4. **Conclusion:** The only genre explicitly stated is \"Shoujo,\" which aligns with the question's requirement.  \n",
      "\n",
      "**Final Answer:**  \n",
      "Nageki no Kenkou Yuuryouji falls under the **Shoujo** genre.\n",
      "</think>\n",
      "The genres that Nageki no Kenkou Yuuryouji falls under can be determined through the provided reference content. \n",
      "\n",
      "**Step-by-step reasoning:**  \n",
      "1. **Analyze the reference content**: The \"Genres\" field explicitly lists three categories:  \n",
      "   - Shoujo  \n",
      "   - Romance  \n",
      "   - Comedy  \n",
      "\n",
      "2. **Confirm no conflicting genres**: The rest of the data (e.g., \"Shoujo\" in the \"Producers\" field) does not contradict or add new genre information.  \n",
      "\n",
      "**Final Answer:**  \n",
      "Nageki no Kenkou Yuuryouji falls under the **Shoujo** genre.\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model_lora, tokenizer = load(\"Qwen/Qwen3-0.6B\", adapter_path=\"adapters\")\n",
    "\n",
    "question = \"Which genres does Nageki no Kenkou Yuuryouji fall under?\"\n",
    "#question = \"What is anime?\"\n",
    "prompt = inference_prompt_style.format(question) # Используем правильный шаблон\n",
    "\n",
    "response = generate(model_lora, tokenizer, prompt=prompt, max_tokens=1200)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141726b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx_anime_qwen3 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
